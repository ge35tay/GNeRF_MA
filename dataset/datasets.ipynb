{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"datasets.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOeJFSITJadzrMjrNijAC2q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0x-Z8rdbY2Bn"},"outputs":[],"source":["__all__ = ['NormalizeForGAN', 'BlendAToRGB', 'Blender', 'DTU']"]},{"cell_type":"code","source":["import os\n","import glob\n","import json\n","import torch\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset"],"metadata":{"id":"26NKy6mGZa-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalize the range of input image from [0, 1] to [-1, 1]\n","class NormalizeForGAN(object):\n","    def __call__(self, x):\n","        return x * 2 - 1\n","    # Returns a string as a representation of the object.\n","    def __repr__(self):\n","        return self.__class__.__name__ + '()'  "],"metadata":{"id":"DH7YGVGWZeQc","executionInfo":{"status":"ok","timestamp":1650314419276,"user_tz":-120,"elapsed":3,"user":{"displayName":"Yinghan Huang","userId":"17583002795918279946"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# blend A to RGB\n","class BlendAToRGB(object):\n","    def __call__(self, x):\n","        if x.shape[0] == 4:\n","            x = x[:3, ...] * x[-1:, ...] + (1 - x[-1:, ...])\n","        return x\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '()'"],"metadata":{"id":"23IoYiI5Z3Xq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Blender(Dataset):\n","    def __init__(self, split, data_dir, img_wh, transforms, sort_key=None):\n","        super(Blender, self).__init__()\n","        self.split = split\n","        self.data_dir = data_dir\n","        self.img_wh = img_wh\n","        self.transforms = transforms\n","        self.sort_key = sort_key\n","\n","        self.filenames = self.get_filenames(self.data_dir)\n","        assert len(self.filenames) > 0, 'File dir is empty'\n","        self.img_wh_original = Image.open(self.filenames[0]).size\n","        assert self.img_wh_original[1] * self.img_wh[0] == self.img_wh_original[0] * self.img_wh[1], \\\n","            f'You must set @img_wh to have the same aspect ratio as ' \\\n","            f'({self.img_wh_original[0]}, {self.img_wh_original[1]}) !'\n","\n","        self.imgs = self.load_imgs(self.filenames)\n","        self.intrinsics, self.poses = self.get_camera_params()\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        img = self.imgs[idx]\n","        return img, idx\n","\n","    def load_imgs(self, filenames):\n","        imgs = []\n","        for p in filenames:\n","            img = Image.open(p)\n","\n","            if self.transforms is not None:\n","                img = self.transforms(img)\n","            imgs.append(img)\n","\n","        return imgs\n","\n","    def get_filenames(self, root):\n","        filenames = glob.glob(f'{root}/{self.split}/*.png')\n","\n","        if self.sort_key is not None:\n","            filenames.sort(key=self.sort_key)\n","        else:\n","            filenames.sort()\n","\n","        if self.split == 'val':  # only validate 8 images\n","            filenames = filenames[:8]\n","\n","        return filenames\n","\n","    def get_camera_params(self):\n","        file_path = os.path.join(self.data_dir, f'transforms_{self.split}.json')\n","\n","        with open(file_path, 'r') as f:\n","            meta = json.load(f)\n","\n","        poses = []\n","        for frame in meta['frames']:\n","            pose = torch.tensor(frame['transform_matrix'])[:3, :4]\n","            poses.append(pose[None])\n","\n","        poses = torch.cat(poses)  # [N, 3, 4]s\n","\n","        cx, cy = [x // 2 for x in self.img_wh_original]\n","        focal = 0.5 * 800 / np.tan(0.5 * meta['camera_angle_x'])  # original focal length\n","\n","        intrinsics = torch.tensor([\n","            [focal, 0, cx], [0, focal, cy], [0, 0, 1.]\n","        ])\n","\n","        scale = torch.tensor([self.img_wh[0] / self.img_wh_original[0], self.img_wh[1] / self.img_wh_original[1]])\n","        intrinsics[:2] *= scale[:, None]\n","\n","        return intrinsics, poses"],"metadata":{"id":"ZXQLNO4HbCtJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class DTU(Dataset):\n","    def __init__(self, split, data_dir, img_wh, transforms, sort_key=None):\n","        super(DTU, self).__init__()\n","        self.split = split\n","        self.data_dir = data_dir\n","        self.img_wh = img_wh\n","        self.transforms = transforms\n","        self.sort_key = sort_key\n","\n","        self.filenames = self.get_filenames(self.data_dir)\n","        assert len(self.filenames) > 0, 'File dir is empty'\n","        self.img_wh_original = Image.open(self.filenames[0]).size\n","        assert self.img_wh_original[1] * self.img_wh[0] == self.img_wh_original[0] * self.img_wh[1], \\\n","            f'You must set @img_wh to have the same aspect ratio as ' \\\n","            f'({self.img_wh_original[0]}, {self.img_wh_original[1]}) !'\n","\n","        self.imgs = self.load_imgs(self.filenames)\n","        self.intrinsics, self.poses = self.get_camera_params()\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        img = self.imgs[idx]\n","        return img, idx\n","\n","    def load_imgs(self, filenames):\n","        imgs = []\n","        for p in filenames:\n","            img = Image.open(p)\n","\n","            if self.transforms is not None:\n","                img = self.transforms(img)\n","            imgs.append(img)\n","\n","        return imgs\n","\n","    def get_filenames(self, data_dir):\n","        filenames = glob.glob(f'{data_dir}/*_3_*.png')  # choose images with a same light condition\n","\n","        if self.sort_key is not None:\n","            filenames.sort(key=self.sort_key)\n","        else:\n","            filenames.sort()\n","\n","        # choose every 8 images as evaluation images, the rest as training images\n","        val_indices = list(np.arange(7, len(filenames), 8))\n","        if self.split == 'train':\n","            filenames = [filenames[x] for x in np.arange(0, len(filenames)) if x not in val_indices]\n","        elif self.split == 'val':\n","            filenames = [filenames[idx] for idx in val_indices]\n","\n","        return filenames\n","\n","    def get_camera_params(self):\n","        prefix = '/'.join(self.data_dir.split('/')[:-2] + ['Cameras', 'train'])\n","        id_list = [os.path.join(prefix, str(int(name.split('/')[-1][5:8]) - 1).zfill(8) + '_cam.txt') for name in\n","                   self.filenames]\n","\n","        intrinsics, poses = [], []\n","        for id in id_list:\n","            with open(id) as f:\n","                text = f.read().splitlines()\n","\n","                pose_text = text[text.index('extrinsic') + 1:text.index('extrinsic') + 5]\n","                pose_text = torch.tensor([[float(b) for b in a.strip().split(' ')] for a in pose_text])\n","                pose_text = torch.inverse(pose_text)\n","\n","                intrinsic_text = text[text.index('intrinsic') + 1:text.index('intrinsic') + 4]\n","                intrinsic_text = torch.tensor([[float(b) for b in a.strip().split(' ')] for a in intrinsic_text])\n","                intrinsic_text[:2, :] *= 4.0  # rescale with image size\n","\n","                poses.append(pose_text[None, :3, :4])\n","                intrinsics.append(intrinsic_text[None])\n","\n","        poses = torch.cat(poses)  # [N, 3, 4]\n","        intrinsics = torch.cat(intrinsics, 0)\n","\n","        intrinsics = intrinsics.mean(dim=0)  # assume intrinsics of all cameras are the same\n","        poses[:, :, 3] /= 200.0\n","\n","        scale = torch.tensor([self.img_wh[0] / self.img_wh_original[0], self.img_wh[1] / self.img_wh_original[1]])\n","        intrinsics[:2] *= scale[:, None]\n","\n","        return intrinsics, poses\n"],"metadata":{"id":"57AFqcx6dRtg"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rendering.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMHWoCXX+HCZKOpF5XoUmwD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NJtJ2R-HilbR"},"outputs":[],"source":["import torch.nn as nn\n","import torch"]},{"cell_type":"code","source":["# Positional Embedding\n","class Embedding(nn.Module):\n","    def __init__(self, in_channels, N_freqs, logscale=True):\n","        \"\"\"\n","        Defines a function that embeds x to (x, sin(2^k x), cos(2^k x), ...)\n","        in_channels: number of input channels (3 for both xyz and direction)\n","        \"\"\"\n","        super(Embedding, self).__init__()\n","        self.N_freqs = N_freqs\n","        self.in_channels = in_channels\n","        self.funcs = [torch.sin, torch.cos]\n","        self.out_channels = in_channels * (len(self.funcs) * N_freqs + 1)\n","\n","        if logscale:\n","            self.freq_bands = 2 ** torch.linspace(0, N_freqs - 1, N_freqs)\n","        else:\n","            self.freq_bands = torch.linspace(1, 2 ** (N_freqs - 1), N_freqs)\n","\n","    def forward(self, x, dim=-1):\n","        \"\"\"\n","        Embeds x to (x, sin(2^k x), cos(2^k x), ...)\n","        Different from the paper, \"x\" is also in the output\n","        See https://github.com/bmild/nerf/issues/12\n","\n","        Inputs:\n","            x: (B, self.in_channels)\n","\n","        Outputs:\n","            out: (B, self.out_channels)\n","        \"\"\"\n","        out = [x]\n","        for freq in self.freq_bands:\n","            for func in self.funcs:\n","                out += [func(freq * x)]\n","\n","        return torch.cat(out, dim)"],"metadata":{"id":"SuHogqg7k3da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample_pdf(bins, weights, N_importance, det=False, eps=1e-5):\n","    \"\"\"\n","    Sample @N_importance samples from @bins with distribution defined by @weights.\n","\n","    Inputs:\n","        bins: (N_rays, N_samples_+1) where N_samples_ is \"the number of coarse samples per ray - 2\"\n","        weights: (N_rays, N_samples_)\n","        N_importance: the number of samples to draw from the distribution\n","        det: deterministic or not\n","        eps: a small number to prevent division by zero\n","\n","    Outputs:\n","        samples: the sampled samples\n","    \"\"\"\n","    N_rays, N_samples_ = weights.shape\n","    weights = weights + eps  # prevent division by zero (don't do inplace op!)\n","    pdf = weights / torch.sum(weights, -1, keepdim=True)  # (N_rays, N_samples_)\n","    cdf = torch.cumsum(pdf, -1)  # (N_rays, N_samples), cumulative distribution function\n","    cdf = torch.cat([torch.zeros_like(cdf[:, :1]), cdf], -1)  # (N_rays, N_samples_+1)\n","    # padded to 0~1 inclusive\n","    # deterministic between 0 and 1\n","    if det:\n","      u = torch.linspace(0, 1, N_importance, device=bins.device)\n","      u = u.expand(N_rays, N_importance)         \n","    else:\n","      u = torch.rand(N_rays, N_importance, device=bins.device)\n","    u = u.contiguous()   # akes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data\n","\n","    inds = torch.searchsorted(cdf.detach(), u, right=True)   # get the position of u in cdf (detached from current computation and will not be updated)\n","    below = torch.clamp_min(inds - 1, 0)\n","    above = torch.clamp_max(inds, N_samples_)\n","\n","    inds_sampled = torch.stack([below, above], -1).view(N_rays, 2 * N_importance)\n","    cdf_g = torch.gather(cdf, 1, inds_sampled).view(N_rays, N_importance, 2)\n","    bins_g = torch.gather(bins, 1, inds_sampled).view(N_rays, N_importance, 2)\n","    denom = cdf_g[..., 1] - cdf_g[..., 0]\n","    denom[denom < eps] = 1  # denom equals 0 means a bin has weight 0, in which case it will not be sampled\n","    # anyway, therefore any value for it is fine (set to 1 here)\n","\n","    samples = bins_g[..., 0] + (u - cdf_g[..., 0]) / denom * (bins_g[..., 1] - bins_g[..., 0])\n","    return samples"],"metadata":{"id":"pyfvAXAvHe2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inference(model, xyz_, dir_, z_vals_, far,\n","              white_back, chunk, noise_std, weights_only=False):\n","    \"\"\"\n","    Helper function that performs model inference.\n","\n","    Inputs:\n","        model: NeRF model (coarse or fine)\n","        xyz_: (N_rays, N_samples_, 3) sampled positions\n","              N_samples_ is the number of sampled points in each ray;\n","                         = N_samples for coarse model\n","                         = N_samples+N_importance for fine model\n","        dir_: (N_rays, 3) ray directions\n","        rays_d: (N_rays, embed_dir_channels) embedded directions\n","        xyz_noise_: (N_rays, s_dim) shape code\n","        dir_noise_: (N_rays, a_dim) appearance code\n","        z_vals: (N_rays, N_samples_) depths of the sampled positions\n","        weights_only: do inference on sigma only or not\n","\n","    Outputs:\n","        if weights_only:\n","            weights: (N_rays, N_samples_): weights of each sample\n","        else:\n","            rgb_final: (N_rays, 3) the final rgb image\n","            depth_final: (N_rays) depth map\n","            weights: (N_rays, N_samples_): weights of each sample\n","    \"\"\"\n","    N_rays, N_samples = xyz_.shape[:2]\n","    rays_d_ = torch.repeat_interleave(dir_, repeats=N_samples, dim=0)  # [N_rays*N_samples, 3]\n","\n","    # Convert these values using volume rendering (Section 4)\n","    xyz_ = xyz_.view(-1, 3)  # [N_rays*N_samples, 4]\n","\n","    deltas = z_vals_[:, 1:] - z_vals_[:, :-1]  # (N_rays, N_samples_-1)\n","    deltas = torch.cat([deltas, far - z_vals_[:, -1:]], -1)  # (N_rays, N_samples_)\n","\n","    # Multiply each distance by the norm of its corresponding direction ray\n","    # to convert to real world distance (accounts for non-unit directions).\n","    deltas = deltas * torch.norm(dir_.unsqueeze(1), dim=-1)\n","\n","    # Perform model inference to get rgb and raw sigma\n","    B = xyz_.shape[0]\n","    out_chunks = []\n","    for i in range(0, B, chunk):\n","        # Embed positions by chunk\n","        xyzdir = torch.cat([xyz_[i:i + chunk], rays_d_[i:i + chunk]], 1)\n","        rgb = model(xyzdir, sigma_only=False)\n","        out_chunks += [rgb]\n","    out_chunks = torch.cat(out_chunks, 0)\n","\n","    if weights_only:\n","        sigmas = out_chunks.view(N_rays, N_samples)\n","    else:\n","        rgbsigma = out_chunks.view(N_rays, N_samples, 4)\n","        rgbs = rgbsigma[..., :3]  # (N_rays, N_samples_, 3)\n","        sigmas = rgbsigma[..., 3]  # (N_rays, N_samples_)\n","\n","    noise = torch.randn(sigmas.shape, device=sigmas.device) * noise_std\n","    # compute alpha by the formula (3)\n","    alphas = 1 - torch.exp(-deltas * torch.relu(sigmas + noise))  # (N_rays, N_samples_)\n","    alphas_shifted = torch.cat([torch.ones_like(alphas[:, :1]), 1 - alphas + 1e-10], -1)  # [1, a1, a2, ...]\n","\n","    T = torch.cumprod(alphas_shifted, -1)\n","    weights = alphas * T[:, :-1]  # (N_rays, N_samples_)\n","    # equals \"1 - (1-a1)(1-a2)...(1-an)\" mathematically\n","\n","    if weights_only:\n","        return weights\n","    else:\n","        # compute final weighted outputs\n","        rgb_final = torch.sum(weights.unsqueeze(-1) * rgbs, -2)  # [N_rays, 3]\n","        depth_final = torch.sum(weights * z_vals_, -1)  # (N_rays)\n","\n","        if white_back:\n","            weights_sum = weights.sum(1)  # (N_rays), the accumulated opacity along the rays\n","            rgb_final = rgb_final + 1 - weights_sum.unsqueeze(-1)\n","\n","        return rgb_final, depth_final, weights"],"metadata":{"id":"QeYhu4MRPEf4"},"execution_count":null,"outputs":[]}]}